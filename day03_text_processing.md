# ğŸ§  Linux å¤§ç¥ä¹‹è·¯ Day 3: æ–‡å­—è™•ç†æŒ‡ä»¤ grep / awk / sed / wc

## âœ¨ ä¸»é¡Œä¸€: å¿«é€Ÿæœå°‹ grep

Data Engineer å¸¸è¦è™•ç†æµæ±—ä¸å·²çš„ logã€è³‡æ–™æ¸…æ½”ã€éŒ¯èª¤è§¸ç™¼ã€‚

### â–¶ï¸ å¸¸è¦‹æ‡‰ç”¨æƒ…å¢ƒ
- æœå°‹ log ä¸­çš„ ERROR
- æ‰¾å‡ºæŒ‡å®šç¯€é»æˆ–æ™‚é–“ç¯€éŒ„
- æ¨è¬€: è™•ç† AWS CloudWatch åŒ¯å‡ºæˆ– Spark log

### âš–ï¸ æŠ€è¡“è§£é‡‹
- `grep` = **Global Regular Expression Print**ï¼Œç”¨ä¾†æœå°‹æ–‡ä»¶ä¸­å«æœ‰æŒ‡å®š pattern çš„è¡Œ
- `-i` = **ignore-case**ï¼Œä¸åˆ†å¤§å°å¯«
- `-v` = **invert-match**ï¼Œåå‘ç¯©é¸ï¼Œæ’é™¤ pattern
- `-A 2 -B 2` = **after/before context**ï¼Œé¡¯ç¤º match å‰å¾Œ 2 è¡Œ
- `-r` = **recursive**ï¼Œéè¿´æœå°‹å…§éƒ¨æ‰€æœ‰æ–‡ä»¶

### ğŸŒ¿ ä½¿ç”¨ç¯„ä¾‹
```bash
grep "ERROR" pipeline.log            # æ‰¾å‡ºå« ERROR çš„è¡Œ
grep -i "error" pipeline.log         # ä¸åˆ†å¤§å°å¯«
grep -v "DEBUG" pipeline.log        # æ’é™¤ DEBUG çš„è¡Œ
grep -A 2 -B 2 "FAIL" result.log     # åˆ—å‡º FAIL å‰å¾Œ 2 è¡Œ
grep -r "spark" /var/log            # éè¿´æœå°‹ spark é—œéµå­—
```

---

## âœ¨ ä¸»é¡ŒäºŒ: æ¡†ç´„å¼è™•ç† awk

`awk` = Alfred Aho, Peter Weinberger, Brian Kernighan åå­—é¦–å­—çµ„æˆã€‚æ˜¯ä¸€å€‹å¾ˆå¼·çš„ pattern scanning & processing å·¥å…·

### â–¶ï¸ å¸¸è¦‹æ‡‰ç”¨æƒ…å¢ƒ
- ç®—å„åœ°å€éŠ·å”®ç¸½é¡
- ç®—å„æ¨£ log ç¨‹åº¦ç‹€æ…‹åˆ†ä½ˆ
- å¾ CSV ä¸­æŠ“ç‰¹å®šæ¬„ä¾†ç•«åœ–

### âš–ï¸ æŠ€è¡“è§£é‡‹
- `awk` ä»¥è¡Œç‚ºå–®ä½ï¼Œå°åˆ— (æ¬„ä½) é€²è¡Œé€²ä¸€æ­¥è™•ç†
- `$1`, `$2`, ... ä»£è¡¨ç¬¬ N æ¬„
- `-F` = **field separator**ï¼ŒæŒ‡å®šæ¬„ä½åˆ†éš”ç¬¦

### ğŸŒ¿ ä½¿ç”¨ç¯„ä¾‹
```bash
awk '{print $1}' result.txt                   # åˆ—å‡ºç¬¬ä¸€æ¬„
awk -F ',' '{print $2, $5}' data.csv          # ä»¥ ',' ç‚ºåˆ†å‰²
awk '$3 > 1000 {print $1, $3}' report.txt     # ç¬¬ä¸‰æ¬„ > 1000
awk '{sum += $4} END {print sum}' sales.csv   # ç¬¬å››æ¬„åŠ çµ
awk '{arr[$2]++} END {for (k in arr) print k, arr[k]}' file.txt  # ç¬¬äºŒæ¬„ groupby 
```

---

## âœ¨ ä¸»é¡Œä¸‰: æ­£å‰‡å°æ› sed

`sed` = **stream editor**ï¼Œç”¨æ–¼è™•ç†æµå¼è¼¸å…¥ä¸­çš„æ–‡å­—ä¸²ä¿®æ”¹

### â–¶ï¸ å¸¸è¦‹æ‡‰ç”¨æƒ…å¢ƒ
- æ‰¹æ¬¡æ”¹æ­£ csv/tsv æ ¼å¼
- æ¸…é™¤æœªç”¨æ–¼è™•ç†è¨˜éŒ„
- é€²è¡Œå­—ä¸²ç¸®å° / å­—é¦–å¤§å°è½‰æ›

### âš–ï¸ æŠ€è¡“è§£é‡‹
- `s/old/new/` = **substitute**ï¼Œå–ä»£
- `/pattern/d` = **delete**ï¼Œåˆªé™¤åŒ¹é…è¡Œ
- `-n` = **no print**ï¼Œä¸è‡ªå‹• printï¼Œé…åˆ `p` æŒ‡å®šåˆ—å°

### ğŸŒ¿ ä½¿ç”¨ç¯„ä¾‹
```bash
sed 's/foo/bar/' config.txt           # åªæ›ç¬¬ä¸€å€‹
sed 's/foo/bar/g' config.txt          # å…¨è¡Œå–ä»£ foo
sed '/# TODO/d' script.sh             # åˆªæ‰ TODO æ‰¹è©•
sed -n '10,20p' app.log               # åˆ—å‡ºç¬¬ 10~20 è¡Œ
```

---

## âœ¨ ä¸»é¡Œå››: å¿«é€Ÿçµ±è¨ˆ wc

å¾ˆå¥½ç”¨çš„ç°¡æ˜“è¨ˆæ•¸å·¥å…·ï¼Œèƒ½æ¥ grep ï¼Œçœ‹å¹¾å€‹é”®è©ã€è¡Œæ•¸ã€èªå¹£ã€‚

### âš–ï¸ æŠ€è¡“è§£é‡‹
- `wc` = **word count**
- `-l` = **line count**ï¼Œè¡Œæ•¸
- `-w` = **word count**ï¼Œå–®è©æ•¸
- `-c` = **character count**ï¼Œå­—å…ƒæ•¸ / bytes

### ğŸŒ¿ ä½¿ç”¨ç¯„ä¾‹
```bash
wc -l app.log                     # è¨ˆç®— app.log æœ‰å¤šå°‘è¡Œ
wc -w README.md                   # å–®è©æ•¸
wc -c data.csv                    # å­—å…ƒ (bytes)
```

---

## ğŸ‹ï¸â€â™‚ï¸ å¯¦æˆ°æƒ…å¢ƒ: åˆ†æ ETL éŒ¯èª¤ log

### æƒ…å¢ƒ: ETL DAG å¤±æ•—ï¼Œéœ€è¦æŠ“å‡ºéŒ¯èª¤èˆ‡å‘½ä»¤é€Ÿåº¦ç®—æ–¹æ¡ˆ

```bash
# æ‰¾éŒ¯èª¤ + å¹¾æ¬¡
grep -c "ERROR" etl.log

# åˆ—å‡ºéŒ¯èª¤ç¯€é»
grep "ERROR" etl.log | awk '{print $1, $2, $6}'

# æœ€å¾Œ 10 æ¬¡éŒ¯èª¤
grep "ERROR" etl.log | tail -10

# éŒ¯èª¤åˆ†æ™‚åˆ†ä½ˆ
grep "ERROR" etl.log | awk '{print substr($2,1,13)}' | sort | uniq -c
```

## ğŸ” Data Engineer é€²éšæ‡‰ç”¨å ´æ™¯

### æ—¥èªŒè§£æèˆ‡ç•°å¸¸æª¢æ¸¬

```bash
# æå–ç‰¹å®šæ™‚é–“ç¯„åœå…§çš„éŒ¯èª¤
sed -n '/2023-04-01 10:00:00/,/2023-04-01 11:00:00/p' app.log | grep "ERROR"

# åˆ†æ Spark åŸ·è¡Œè¨ˆåŠƒä¸­çš„ Stage è€—æ™‚
grep "Stage" spark.log | awk '{print $1, $4, $NF}' | sort -k3 -nr | head -10

# ç›£æ§ç‰¹å®šä»»å‹™çš„è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
grep "Memory usage" app.log | awk '{print $1, $2, $(NF-1), $NF}' | tail -100 > memory_trend.txt
```

### æ•¸æ“šæ¸…æ´—èˆ‡è½‰æ›

```bash
# CSV æ–‡ä»¶åˆ—è½‰æ›ï¼ˆä¾‹å¦‚æ—¥æœŸæ ¼å¼æ¨™æº–åŒ–ï¼‰
awk -F, '{gsub(/(\d{2})\/(\d{2})\/(\d{4})/, "\\3-\\1-\\2", $3); print}' data.csv > cleaned_data.csv

# ç§»é™¤ CSV ä¸­çš„å¼•è™Ÿå’Œç©ºæ ¼
sed 's/"//g' data.csv | sed 's/^ *//g; s/ *$//g' > clean_data.csv

# åˆä½µå¤šå€‹æ—¥èªŒæ–‡ä»¶ä¸¦æå–é—œéµä¿¡æ¯
find /var/log/app/ -name "*.log" -type f -exec grep "Transaction" {} \; | awk '{print $1, $2, $9, $12}' > transactions_summary.txt
```

### æ€§èƒ½åˆ†æ

```bash
# æå– API éŸ¿æ‡‰æ™‚é–“ä¸¦è¨ˆç®—å¹³å‡å€¼ã€æœ€å¤§å€¼å’Œåˆ†ä½æ•¸
grep "API response time" app.log | awk '{sum+=$NF; count++; arr[NR]=$NF} END {print "Avg:", sum/count, "Max:", arr[int(count*0.95)], "95th:", arr[int(count*0.95)], "99th:", arr[int(count*0.99)]}'

# åˆ†ææŸ¥è©¢åŸ·è¡Œæ™‚é–“åˆ†ä½ˆ
grep "Query completed" db.log | awk '{print $7}' | sort -n | awk '
BEGIN {count=0}
{
  sum+=$1; count++; data[count]=$1
}
END {
  print "Min:", data[1]
  print "Max:", data[count]
  print "Avg:", sum/count
  print "Median:", data[int(count/2)]
  print "90th:", data[int(count*0.9)]
  print "95th:", data[int(count*0.95)]
  print "99th:", data[int(count*0.99)]
}'
```

### æ•¸æ“šè³ªé‡æª¢æŸ¥

```bash
# æª¢æŸ¥ CSV æ–‡ä»¶åˆ—æ•¸æ˜¯å¦ä¸€è‡´
awk -F, '{print NF}' data.csv | sort | uniq -c

# æª¢æŸ¥æ•¸æ“šä¸­çš„ NULL å€¼æˆ–ç©ºå­—æ®µ
awk -F, '{for(i=1;i<=NF;i++) if($i=="NULL" || $i=="") null_count[i]++} END {for(i in null_count) print "Column", i, "has", null_count[i], "NULL values"}' data.csv

# æª¢æŸ¥æ•¸å€¼åˆ—çš„ç¯„åœå’Œåˆ†ä½ˆ
awk -F, '{if(NR>1) {sum+=$3; count++; if($3>max) max=$3; if($3<min || min=="") min=$3}} END {print "Min:", min, "Max:", max, "Avg:", sum/count}' data.csv
```

### ETL æµç¨‹ç›£æ§èˆ‡è‡ªå‹•åŒ–

```bash
# ç›£æ§ ETL ä½œæ¥­å®Œæˆæƒ…æ³ä¸¦ç™¼é€é€šçŸ¥
grep "Job completed" etl.log | awk '{
  if($NF=="SUCCESS") success++; 
  else if($NF=="FAILED") failed++
} END {
  print "Success:", success, "Failed:", failed
  if(failed>0) exit 1
}'

# å¦‚æœä¸Šä¸€å€‹å‘½ä»¤è¿”å›éé›¶ï¼Œç™¼é€è­¦å ±
if [ $? -ne 0 ]; then
  echo "ETL jobs failed, check logs" | mail -s "ETL Alert" data-team@example.com
fi

# è‡ªå‹•ç”Ÿæˆæ¯æ—¥ ETL å ±å‘Š
{
  echo "=== ETL Daily Report $(date +%F) ==="
  echo "Jobs executed: $(grep "Job started" etl.log | wc -l)"
  echo "Success rate: $(grep "Job completed" etl.log | grep "SUCCESS" | wc -l) / $(grep "Job completed" etl.log | wc -l)"
  echo "Average runtime: $(grep "Runtime" etl.log | awk '{sum+=$NF; count++} END {print sum/count " seconds"}')" 
  echo "Slowest job: $(grep "Runtime" etl.log | sort -k3 -nr | head -1)"
  echo "Data processed: $(grep "Records processed" etl.log | awk '{sum+=$NF} END {print sum " records"}')" 
} > etl_report_$(date +%F).txt
```

### å¤§æ•¸æ“šå¹³å°ç‰¹å®šå ´æ™¯

```bash
# åˆ†æ Hadoop/HDFS ç©ºé–“ä½¿ç”¨æƒ…æ³
hdfs dfs -du -h /data | sort -hr | head -10 | awk '{print $1, $2}'

# æå– Spark æ‡‰ç”¨ç¨‹åºçš„è³‡æºä½¿ç”¨æƒ…æ³
grep "executor" spark.log | awk '/memory|cores/ {print $1, $2, $(NF-1), $NF}'

# ç›£æ§ Kafka æ¶ˆè²»å»¶é²
grep "consumer lag" kafka.log | awk '{print $1, $2, $8}' | sort -k3 -nr | head
```

---

## âœ¨ ä¸»é¡Œäº”: çµ„åˆæ‡‰ç”¨ & åŠ å€¼å¯¦æˆ°

### æƒ…å¢ƒ: æ‰¾å‡ºæŸ¥è©¢éŒ¯èª¤ + å¹³å‡ç®—çµ

```bash
# æ‰¾å‡ºæœ€å¤šéŒ¯èª¤ä¾†æº IP
awk '$9 ~ /500/' access.log | awk '{print $1}' | sort | uniq -c | sort -rn | head

# ç®—å„ç¨® HTTP status code åˆ†ä½ˆ
awk '{print $9}' access.log | sort | uniq -c | sort -rn

# å°‡ response time > 3s çš„é …ç›®åˆ—å‡º
awk '$10 > 3000 {print $7, $10}' access.log | sort -nk2 | tail -5
```

---

## â­ Day 3 æŠ€èƒ½çŸ©é™£

| æŠ€è¡“ | èƒ½åŠ› |
|--------|------|
| `grep` | é—œéµå­—/éŒ¯èª¤æœå°‹  |
| `awk`  | æ¬„ä½ç®—çµ/åˆ†ç¾¤çµ±è¨ˆ |
| `sed`  | æ–‡å­—æ‰¹æ¬¡è®Šæ›´ |
| `wc`   | è¨ˆæ•¸ (è¡Œæ•¸/å­—å…ƒ/è©æ•¸) |

---

## âœ… å»¶ä¼¸ä»»å‹™ï¼ˆDay3 Bonusï¼‰

1. åšå‡ºä¸€å€‹ CSV åˆ†æå°ç…§è¡¨
2. è‡ªå‹•æŠŠ ETL éŒ¯èª¤ grep å‡ºä¾†åŠ ä¸Šæ™‚é–“
3. ä½¿ç”¨ awk + sed åšæˆ summary report
4. çµ„åˆ grep + awk + sort æ‰“é€ ä¸è¼¸ BI çš„ CLI è§€æ¸¬å…‰æ¨¹

---

## ğŸ§ª å¯¦éš›ç·´ç¿’ç¯„ä¾‹

åœ¨ `data/` ç›®éŒ„ä¸­æœ‰ä¸‰å€‹æ¨¡æ“¬è³‡æ–™æª”æ¡ˆå¯ä¾›ç·´ç¿’ï¼š

1. `sales_data.csv` - éŠ·å”®è³‡æ–™
2. `server_logs.txt` - ä¼ºæœå™¨æ—¥èªŒ
3. `access_log.txt` - ç¶²ç«™è¨ªå•æ—¥èªŒ

### ç·´ç¿’ä¸€ï¼šè™•ç†éŠ·å”®è³‡æ–™ (sales_data.csv)

```bash
# æª¢è¦–è³‡æ–™çµæ§‹
head -1 data/sales_data.csv
# è¼¸å‡º: region,date,product,quantity,price,total
```

**å‘½ä»¤è§£é‡‹ï¼š**
- `head`: é¡¯ç¤ºæ–‡ä»¶é–‹é ­éƒ¨åˆ†
- `-1`: åªé¡¯ç¤ºç¬¬ä¸€è¡Œ
- `data/sales_data.csv`: ç›®æ¨™æ–‡ä»¶è·¯å¾‘

```bash
# è¨ˆç®—å„å€åŸŸéŠ·å”®ç¸½é¡
awk -F, 'NR>1 {sum[$1]+=$6} END {for (region in sum) print region, sum[region]}' data/sales_data.csv
# è¼¸å‡ºçµæœæœƒé¡¯ç¤ºæ¯å€‹å€åŸŸçš„ç¸½éŠ·å”®é¡
```

**å‘½ä»¤è§£é‡‹ï¼š**
- `awk`: æ–‡æœ¬è™•ç†å·¥å…·
- `-F,`: è¨­ç½®æ¬„ä½åˆ†éš”ç¬¦ç‚ºé€—è™Ÿ
- `NR>1`: Number of Record > 1ï¼Œè·³éç¬¬ä¸€è¡Œï¼ˆæ¨™é¡Œè¡Œï¼‰
- `{sum[$1]+=$6}`: å°æ¯ä¸€è¡ŒåŸ·è¡Œçš„æ“ä½œï¼Œ`$1`æ˜¯ç¬¬ä¸€åˆ—(å€åŸŸ)ï¼Œ`$6`æ˜¯ç¬¬å…­åˆ—(ç¸½é¡)ï¼Œå°‡æ¯å€‹å€åŸŸçš„éŠ·å”®é¡ç´¯åŠ åˆ°`sum`æ•¸çµ„ä¸­
- `END {...}`: åœ¨è™•ç†å®Œæ‰€æœ‰è¡Œå¾ŒåŸ·è¡Œçš„ä»£ç¢¼å¡Š
- `for (region in sum)`: éæ­·`sum`æ•¸çµ„ä¸­çš„æ‰€æœ‰éµ(å€åŸŸ)
- `print region, sum[region]`: æ‰“å°å€åŸŸåå’Œå°æ‡‰çš„ç¸½éŠ·å”®é¡

```bash
# æŸ¥æ‰¾éŠ·å”®é¡è¶…é100000çš„è¨˜éŒ„
awk -F, '$6 > 100000 {print $1, $3, $6}' data/sales_data.csv
# è¼¸å‡ºæœƒé¡¯ç¤ºé«˜éŠ·å”®é¡çš„å€åŸŸã€ç”¢å“å’Œé‡‘é¡
```

**å‘½ä»¤è§£é‡‹ï¼š**
- `-F,`: è¨­ç½®æ¬„ä½åˆ†éš”ç¬¦ç‚ºé€—è™Ÿ
- `$6 > 100000`: æ¢ä»¶è¡¨é”å¼ï¼Œåªè™•ç†ç¬¬å…­åˆ—(ç¸½é¡)å¤§æ–¼100000çš„è¡Œ
- `{print $1, $3, $6}`: æ‰“å°ç¬¬ä¸€åˆ—(å€åŸŸ)ã€ç¬¬ä¸‰åˆ—(ç”¢å“)å’Œç¬¬å…­åˆ—(ç¸½é¡)

```bash
# è¨ˆç®—æ¯ç¨®ç”¢å“çš„ç¸½éŠ·å”®é‡
awk -F, 'NR>1 {qty[$3]+=$4} END {for (product in qty) print product, qty[product]}' data/sales_data.csv
# è¼¸å‡ºæœƒé¡¯ç¤ºæ¯ç¨®ç”¢å“çš„ç¸½éŠ·å”®æ•¸é‡
```

**å‘½ä»¤è§£é‡‹ï¼š**
- `NR>1`: è·³éç¬¬ä¸€è¡Œ
- `{qty[$3]+=$4}`: å°‡æ¯ç¨®ç”¢å“($3)çš„éŠ·å”®é‡($4)ç´¯åŠ åˆ°`qty`æ•¸çµ„ä¸­
- `END {...}`: è™•ç†å®Œæ‰€æœ‰è¡Œå¾ŒåŸ·è¡Œ
- `for (product in qty)`: éæ­·æ‰€æœ‰ç”¢å“
- `print product, qty[product]`: æ‰“å°ç”¢å“åå’Œå°æ‡‰çš„ç¸½éŠ·å”®é‡

### ç·´ç¿’äºŒï¼šåˆ†æä¼ºæœå™¨æ—¥èªŒ (server_logs.txt)

```bash
# çµ±è¨ˆå„æ—¥èªŒç´šåˆ¥çš„æ•¸é‡
awk '{print $3}' data/server_logs.txt | sort | uniq -c
# è¼¸å‡ºæœƒé¡¯ç¤º INFOã€DEBUGã€ERRORã€WARN ç­‰ç´šåˆ¥çš„æ•¸é‡
```

**å‘½ä»¤è§£é‡‹ï¼š**
- `{print $3}`: æ‰“å°æ¯è¡Œçš„ç¬¬ä¸‰åˆ—(æ—¥èªŒç´šåˆ¥)
- `|`: ç®¡é“ç¬¦ï¼Œå°‡å‰ä¸€å€‹å‘½ä»¤çš„è¼¸å‡ºä½œç‚ºå¾Œä¸€å€‹å‘½ä»¤çš„è¼¸å…¥
- `sort`: å°è¼¸å‡ºé€²è¡Œæ’åº
- `uniq -c`: å»é™¤é‡è¤‡è¡Œä¸¦è¨ˆæ•¸ï¼Œ`-c`è¡¨ç¤ºé¡¯ç¤ºè¨ˆæ•¸

```bash
# æå–æ‰€æœ‰éŒ¯èª¤æ—¥èªŒ
awk '$3 == "ERROR" {print $1, $2, $4, $5, $6, $7, $8, $9}' data/server_logs.txt
# è¼¸å‡ºæœƒé¡¯ç¤ºæ‰€æœ‰éŒ¯èª¤æ—¥èªŒçš„è©³ç´°ä¿¡æ¯
```

**å‘½ä»¤è§£é‡‹ï¼š**
- `$3 == "ERROR"`: æ¢ä»¶è¡¨é”å¼ï¼Œåªè™•ç†ç¬¬ä¸‰åˆ—ç­‰æ–¼"ERROR"çš„è¡Œ
- `{print $1, $2, $4, $5, $6, $7, $8, $9}`: æ‰“å°æŒ‡å®šåˆ—(æ—¥æœŸã€æ™‚é–“å’ŒéŒ¯èª¤è©³æƒ…)

```bash
# è¨ˆç®—å¹³å‡è¨˜æ†¶é«”ä½¿ç”¨é‡
awk '/Memory usage/ {sum += $5; count++} END {print "Average memory usage:", sum/count "GB"}' data/server_logs.txt
# è¼¸å‡ºæœƒé¡¯ç¤ºå¹³å‡è¨˜æ†¶é«”ä½¿ç”¨é‡
```

**å‘½ä»¤è§£é‡‹ï¼š**
- `/Memory usage/`: æ¨¡å¼åŒ¹é…ï¼Œåªè™•ç†åŒ…å«"Memory usage"çš„è¡Œ
- `{sum += $5; count++}`: ç´¯åŠ ç¬¬äº”åˆ—(å…§å­˜ä½¿ç”¨é‡)ä¸¦è¨ˆæ•¸
- `END {...}`: è™•ç†å®Œæ‰€æœ‰è¡Œå¾ŒåŸ·è¡Œ
- `print "Average memory usage:", sum/count "GB"`: æ‰“å°å¹³å‡å…§å­˜ä½¿ç”¨é‡

```bash
# æŒ‰å°æ™‚çµ±è¨ˆæ—¥èªŒæ•¸é‡
awk '{split($2,t,":"); count[substr($1,1,13) "-" t[1]]++} END {for (hour in count) print hour, count[hour]}' data/server_logs.txt | sort
# è¼¸å‡ºæœƒé¡¯ç¤ºæ¯å°æ™‚çš„æ—¥èªŒæ•¸é‡
```

**å‘½ä»¤è§£é‡‹ï¼š**
- `split($2,t,":")`: å°‡ç¬¬äºŒåˆ—(æ™‚é–“)æŒ‰å†’è™Ÿåˆ†å‰²ï¼Œå­˜å…¥æ•¸çµ„`t`
- `count[substr($1,1,13) "-" t[1]]++`: å°‡æ—¥æœŸ($1)çš„å‰13å€‹å­—ç¬¦(å¹´æœˆæ—¥)å’Œå°æ™‚(t[1])çµ„åˆä½œç‚ºéµï¼Œå°æ‡‰çš„å€¼åŠ 1
- `END {...}`: è™•ç†å®Œæ‰€æœ‰è¡Œå¾ŒåŸ·è¡Œ
- `for (hour in count)`: éæ­·æ‰€æœ‰å°æ™‚
- `print hour, count[hour]`: æ‰“å°å°æ™‚å’Œå°æ‡‰çš„æ—¥èªŒæ•¸é‡
- `| sort`: å°çµæœé€²è¡Œæ’åº

### ç·´ç¿’ä¸‰ï¼šåˆ†æç¶²ç«™è¨ªå•æ—¥èªŒ (access_log.txt)

```bash
# çµ±è¨ˆå„ HTTP ç‹€æ…‹ç¢¼çš„æ•¸é‡
awk '{print $9}' data/access_log.txt | sort | uniq -c
# è¼¸å‡ºæœƒé¡¯ç¤ºå„ç‹€æ…‹ç¢¼ï¼ˆå¦‚ 200ã€404ã€500 ç­‰ï¼‰çš„æ•¸é‡
```

**å‘½ä»¤è§£é‡‹ï¼š**
- `{print $9}`: æ‰“å°ç¬¬ä¹åˆ—(HTTPç‹€æ…‹ç¢¼)
- `| sort`: å°è¼¸å‡ºé€²è¡Œæ’åº
- `| uniq -c`: å»é™¤é‡è¤‡è¡Œä¸¦è¨ˆæ•¸

```bash
# æ‰¾å‡ºéŸ¿æ‡‰æ™‚é–“è¶…é 1000ms çš„è«‹æ±‚
awk '$NF > 1000 {print $1, $7, $NF}' data/access_log.txt
# è¼¸å‡ºæœƒé¡¯ç¤ºéŸ¿æ‡‰æ™‚é–“è¼ƒé•·çš„è«‹æ±‚çš„ IPã€URL å’ŒéŸ¿æ‡‰æ™‚é–“
```

**å‘½ä»¤è§£é‡‹ï¼š**
- `$NF`: è¡¨ç¤ºæœ€å¾Œä¸€åˆ—(éŸ¿æ‡‰æ™‚é–“)ï¼ŒNF = Number of Fields
- `$NF > 1000`: æ¢ä»¶è¡¨é”å¼ï¼Œåªè™•ç†éŸ¿æ‡‰æ™‚é–“å¤§æ–¼1000msçš„è¡Œ
- `{print $1, $7, $NF}`: æ‰“å°ç¬¬ä¸€åˆ—(IP)ã€ç¬¬ä¸ƒåˆ—(URL)å’Œæœ€å¾Œä¸€åˆ—(éŸ¿æ‡‰æ™‚é–“)

```bash
# è¨ˆç®—æ¯å€‹ IP çš„è«‹æ±‚æ¬¡æ•¸
awk '{count[$1]++} END {for (ip in count) print ip, count[ip]}' data/access_log.txt
# è¼¸å‡ºæœƒé¡¯ç¤ºæ¯å€‹ IP åœ°å€çš„è«‹æ±‚æ¬¡æ•¸
```

**å‘½ä»¤è§£é‡‹ï¼š**
- `{count[$1]++}`: å°‡æ¯å€‹ IP($1)çš„è¨ˆæ•¸åŠ 1
- `END {...}`: è™•ç†å®Œæ‰€æœ‰è¡Œå¾ŒåŸ·è¡Œ
- `for (ip in count)`: éæ­·æ‰€æœ‰IP
- `print ip, count[ip]`: æ‰“å°IPå’Œå°æ‡‰çš„è«‹æ±‚æ¬¡æ•¸

```bash
# åˆ†æè¨ªå•æœ€å¤šçš„ URL
awk '{urls[$7]++} END {for (url in urls) print urls[url], url}' data/access_log.txt | sort -nr | head -5
# è¼¸å‡ºæœƒé¡¯ç¤ºè¨ªå•é‡æœ€å¤§çš„ 5 å€‹ URL
```

**å‘½ä»¤è§£é‡‹ï¼š**
- `{urls[$7]++}`: å°‡æ¯å€‹ URL($7)çš„è¨ˆæ•¸åŠ 1
- `END {...}`: è™•ç†å®Œæ‰€æœ‰è¡Œå¾ŒåŸ·è¡Œ
- `for (url in urls)`: éæ­·æ‰€æœ‰URL
- `print urls[url], url`: æ‰“å°è¨ªå•æ¬¡æ•¸å’ŒURL(æ³¨æ„é †åºï¼Œå…ˆæ‰“å°æ¬¡æ•¸ä¾¿æ–¼æ’åº)
- `| sort -nr`: å°çµæœé€²è¡Œæ•¸å­—é€†åºæ’åºï¼Œ`-n`è¡¨ç¤ºæŒ‰æ•¸å­—æ’åºï¼Œ`-r`è¡¨ç¤ºé€†åº
- `| head -5`: åªé¡¯ç¤ºå‰5è¡Œçµæœ

### ç¶œåˆç·´ç¿’ï¼šå¤šå‘½ä»¤çµ„åˆ

```bash
# ç”Ÿæˆæ¯å°æ™‚éŒ¯èª¤ç‡å ±å‘Š
awk '{
  total[$1]++;
  if ($9 >= 400) errors[$1]++
} END {
  print "Hour,Total,Errors,ErrorRate";
  for (hour in total) {
    if (!errors[hour]) errors[hour]=0;
    printf "%s,%d,%d,%.2f%%\n", hour, total[hour], errors[hour], (errors[hour]/total[hour])*100
  }
}' data/access_log.txt | sort > error_rate_report.csv
```

**å‘½ä»¤è§£é‡‹ï¼š**
- `total[$1]++`: å°‡æ¯å€‹ IP çš„è«‹æ±‚ç¸½æ•¸åŠ 1
- `if ($9 >= 400) errors[$1]++`: å¦‚æœç‹€æ…‹ç¢¼å¤§æ–¼ç­‰æ–¼400ï¼Œå°‡è©² IP çš„éŒ¯èª¤æ•¸åŠ 1
- `END {...}`: è™•ç†å®Œæ‰€æœ‰è¡Œå¾ŒåŸ·è¡Œ
- `print "Hour,Total,Errors,ErrorRate"`: æ‰“å° CSV æ¨™é¡Œè¡Œ
- `if (!errors[hour]) errors[hour]=0`: å¦‚æœæŸå°æ™‚æ²’æœ‰éŒ¯èª¤ï¼Œå°‡éŒ¯èª¤æ•¸è¨­ç‚º0
- `printf "%s,%d,%d,%.2f%%\n"`: æ ¼å¼åŒ–è¼¸å‡ºï¼Œ`%s`ç‚ºå­—ç¬¦ä¸²ï¼Œ`%d`ç‚ºæ•´æ•¸ï¼Œ`%.2f%%`ç‚ºå¸¶å…©ä½å°æ•¸çš„ç™¾åˆ†æ¯”
- `| sort > error_rate_report.csv`: æ’åºä¸¦å°‡çµæœå¯«å…¥ CSV æ–‡ä»¶

```bash
# æ‰¾å‡ºä¸¦åˆ†ææ…¢æŸ¥è©¢
grep "Query executed" data/server_logs.txt | \
  sed -E 's/.*in ([0-9]+)ms: (.*)/\1 \2/' | \
  sort -nr | head -5
# è¼¸å‡ºæœƒé¡¯ç¤ºæœ€æ…¢çš„ 5 å€‹æŸ¥è©¢åŠå…¶åŸ·è¡Œæ™‚é–“
```

**å‘½ä»¤è§£é‡‹ï¼š**
- `grep "Query executed"`: æ‰¾å‡ºåŒ…å«"Query executed"çš„è¡Œ
- `\`: è¡ŒçºŒç¬¦ï¼Œè¡¨ç¤ºå‘½ä»¤åœ¨ä¸‹ä¸€è¡Œç¹¼çºŒ
- `sed -E 's/.*in ([0-9]+)ms: (.*)/\1 \2/'`: ä½¿ç”¨æ“´å±•æ­£å‰‡è¡¨é”å¼æå–åŸ·è¡Œæ™‚é–“å’ŒæŸ¥è©¢å…§å®¹
  - `-E`: å•Ÿç”¨æ“´å±•æ­£å‰‡è¡¨é”å¼
  - `.*in ([0-9]+)ms: (.*)`: åŒ¹é…æ¨¡å¼ï¼Œæ•æ‰æ•¸å­—å’ŒæŸ¥è©¢å…§å®¹
  - `\1 \2`: æ›¿æ›ç‚ºæ•æ‰çš„å…§å®¹ï¼Œæ™‚é–“å’ŒæŸ¥è©¢
- `sort -nr`: æŒ‰æ•¸å­—é€†åºæ’åºï¼Œå°‡æœ€å¤§çš„æ•¸å­—ï¼ˆæœ€æ…¢çš„æŸ¥è©¢ï¼‰æ”¾åœ¨å‰é¢
- `head -5`: åªé¡¯ç¤ºå‰5è¡Œçµæœ

é€™äº›ç·´ç¿’ç¯„ä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Linux æ–‡å­—è™•ç†å·¥å…·åˆ†æä¸åŒé¡å‹çš„è³‡æ–™ï¼Œå¾éŠ·å”®æ•¸æ“šåˆ°ä¼ºæœå™¨æ—¥èªŒå†åˆ°ç¶²ç«™è¨ªå•è¨˜éŒ„ã€‚é€šéçµ„åˆä½¿ç”¨ grepã€awkã€sed å’Œå…¶ä»–å‘½ä»¤ï¼Œä½ å¯ä»¥åŸ·è¡Œè¤‡é›œçš„è³‡æ–™åˆ†æä»»å‹™ï¼Œè€Œç„¡éœ€ä¾è³´å°ˆé–€çš„åˆ†æå·¥å…·ã€‚

---

